{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ABDSfhm-F3R1",
        "htSRtMn4F__C"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "ABDSfhm-F3R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eCOHxNdtZ7ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyvblDRbycf9"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hxpQYEl6ylH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define constants"
      ],
      "metadata": {
        "id": "htSRtMn4F__C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "W3Ts1JVUylEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE= 8\n",
        "PIN_MEMORY= True if DEVICE.type == \"cuda\" else False"
      ],
      "metadata": {
        "id": "TYae6yMrylCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs= 20\n",
        "lr= 1e-4\n",
        "d_model= 512\n",
        "d_ff= 2048\n",
        "src_lang= \"en\"\n",
        "trg_lang= \"hi\"\n",
        "weight_decay= 1e-5\n",
        "TRAIN_CONSOLE_CHECKPOINT= 300\n",
        "VALID_CONSOLE_CHECKPOINT= 55"
      ],
      "metadata": {
        "id": "2-15F4QUDmlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components Architecture"
      ],
      "metadata": {
        "id": "9QM0UETLGMm2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzFCz2NYyk__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Input embeddings:-"
      ],
      "metadata": {
        "id": "fjc6kLAKGR2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Define a custom embedding layer for input tokens\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Create an embedding layer that maps token indices to vectors of size d_model\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,   # Total number of tokens in vocabulary\n",
        "            embedding_dim=d_model        # Size of each embedding vector\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Scale the embedding by sqrt(d_model) as done in Transformer architecture\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "hYfGnYUdyk9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Y9Rpgroyk7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Positional encodings"
      ],
      "metadata": {
        "id": "G-TiBOHKWx3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Positional encoding module adds information about the position of each token\n",
        "class PositinalEncodings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, seq_len, dropout_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Initialize positional encoding tensor of shape (seq_len, d_model)\n",
        "        pe = torch.zeros((seq_len, d_model)).to(DEVICE)  # Shape: (seq_len, d_model)\n",
        "\n",
        "        # Create position indices (0 to seq_len-1), shape: (seq_len, 1)\n",
        "        positions = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # Compute the denominator term for sinusoidal functions (only for even indices)\n",
        "        even_indexes = torch.arange(0, d_model, 2)\n",
        "        div_term = 10000 ** (-even_indexes / d_model)\n",
        "\n",
        "        # Apply sine to even indices in the embedding dimension\n",
        "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
        "\n",
        "        # Apply cosine to odd indices in the embedding dimension\n",
        "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
        "\n",
        "        # Add a batch dimension: (1, seq_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register 'pe' as a buffer so it’s not updated by gradients but saved with the model\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to input tensor `x`\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        pos = x + self.pe[:, :x.shape[1], :].requires_grad_(False)\n",
        "\n",
        "        return self.dropout(pos)\n"
      ],
      "metadata": {
        "id": "TCDjveuayk5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "As6XGEynyk0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Feed forward NN"
      ],
      "metadata": {
        "id": "W5hqaqFaaKeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Position-wise Feed-Forward Neural Network used in Transformer blocks\n",
        "class FeedForwardNN(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=d_model,   # Input dimension\n",
        "                out_features=d_ff      # Hidden dimension\n",
        "            ),\n",
        "\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(\n",
        "                in_features=d_ff,      # Hidden dimension\n",
        "                out_features=d_model   # Output dimension (same as input)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n"
      ],
      "metadata": {
        "id": "Sb7FkMubykxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ur0Uv3zCykvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Multi-head attention"
      ],
      "metadata": {
        "id": "HPS_btLzh7x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, dropout_rate):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model= d_model\n",
        "    self.num_heads= num_heads\n",
        "\n",
        "    # Ensure d_model is divisible by the number of heads\n",
        "    assert (d_model % num_heads == 0), f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n",
        "\n",
        "\n",
        "    # Calculate the dimension of each attention head\n",
        "    self.head_dims= d_model // num_heads\n",
        "\n",
        "    self.q_dense= nn.Linear(\n",
        "        in_features= d_model,\n",
        "        out_features= d_model\n",
        "    ) # Wq\n",
        "\n",
        "    self.k_dense= nn.Linear(\n",
        "        in_features= d_model,\n",
        "        out_features= d_model\n",
        "    ) # Wk\n",
        "\n",
        "    self.v_dense= nn.Linear(\n",
        "        in_features= d_model,\n",
        "        out_features= d_model\n",
        "    ) # Wv\n",
        "\n",
        "    self.o_dense= nn.Linear(\n",
        "        in_features= d_model,\n",
        "        out_features= d_model\n",
        "    ) # Wo\n",
        "\n",
        "    self.dropout= nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, values, mask, dropout):\n",
        "\n",
        "    # queries, keys, values -> (B, num_heads, seq_len, head_dims)\n",
        "    attention_scores= query @ key.transpose(-2, -1) # (B, num_heads, seq_len, seq_len)\n",
        "    attention_scores/= math.sqrt(query.shape[-1])\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores= nn.Softmax(dim= -1)(attention_scores)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores= dropout(attention_scores)\n",
        "\n",
        "    output= attention_scores @ values # (B, num_heads, seq_len, head_dims)\n",
        "\n",
        "    return output, attention_scores\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "\n",
        "    # x -> (seq_len, d_model)\n",
        "    queries= self.q_dense(q) # (seq_len, d_model)\n",
        "    keys= self.k_dense(k) # (seq_len, d_model)\n",
        "    values= self.v_dense(v) # (seq_len, d_model)\n",
        "\n",
        "    # reshape := (seq_len, d_model) -> (seq_len, num_heads, head_dims)\n",
        "    queries= queries.view(queries.shape[0], queries.shape[1], self.num_heads, self.head_dims)\n",
        "    keys= keys.view(keys.shape[0], keys.shape[1], self.num_heads, self.head_dims)\n",
        "    values= values.view(values.shape[0], values.shape[1], self.num_heads, self.head_dims)\n",
        "\n",
        "    # reshape := (seq_len, num_heads, head_dims) -> (num_heads, seq_len, head_dims)\n",
        "    queries= queries.transpose(1, 2)\n",
        "    keys= keys.transpose(1, 2)\n",
        "    values= values.transpose(1, 2)\n",
        "\n",
        "\n",
        "    # get attention outputs\n",
        "    output, attention_scores= MultiHeadAttention.attention(queries, keys, values, mask, self.dropout)\n",
        "\n",
        "    # output -> (B, num_heads, seq_len, head_dims)\n",
        "    output= output.transpose(1, 2) # (B, seq_len, num_heads, head_dims)\n",
        "\n",
        "    output= output.contiguous().view(output.shape[0], -1, self.num_heads * self.head_dims) # (B, seq_len, d_model)\n",
        "\n",
        "    return self.o_dense(output) # (B, seq_len, d_model)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZuIssT8Lh7k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B0ymYIf4h7fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Encoder"
      ],
      "metadata": {
        "id": "i7s49pYhuf9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, num_heads, dropout_rate):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi-Head Self-Attention layer\n",
        "    self.multi_head_attention= MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "\n",
        "    # Feed Forward Neural Network layer\n",
        "    self.feed_forward_nn= FeedForwardNN(d_model, d_ff, dropout_rate)\n",
        "\n",
        "    # Layer normalization (used twice: after attention and FFN)\n",
        "    self.norm= nn.LayerNorm(\n",
        "        normalized_shape= d_model\n",
        "    )\n",
        "\n",
        "    self.dropout= nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    # Step 1: Multi-Head Self-Attention + Add & Norm\n",
        "    attention_output= self.multi_head_attention(x, x, x, mask)\n",
        "    attention_add_norm_output= self.norm(x + self.dropout(attention_output))\n",
        "\n",
        "\n",
        "    # Step 2: Feed Forward Network + Add & Norm\n",
        "    nn_output= self.feed_forward_nn(attention_add_norm_output)\n",
        "    nn_add_norm_out= self.norm(attention_add_norm_output + self.dropout(nn_output))\n",
        "\n",
        "    return nn_add_norm_out\n",
        "\n"
      ],
      "metadata": {
        "id": "n9YJLGzuyks9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, num_heads, dropout_rate, num_encoder_blocks):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    # Stack multiple encoder blocks using ModuleList\n",
        "    self.encoder_blocks= nn.ModuleList([])\n",
        "\n",
        "    for _ in range(num_encoder_blocks):\n",
        "      self.encoder_blocks.append(\n",
        "          EncoderBlock(d_model, d_ff, num_heads, dropout_rate)\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    # Pass input through each encoder block sequentially\n",
        "    for block in self.encoder_blocks:\n",
        "      x= block(x, mask)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "4gGClqJzuh6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSZkLIgIuh3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Decoder"
      ],
      "metadata": {
        "id": "L0Hxjkxj04hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, num_heads, dropout_rate):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # Masked self-attention for the decoder\n",
        "    self.self_attention= MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "\n",
        "     # Cross-attention over the encoder output\n",
        "    self.cross_attention= MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
        "\n",
        "    # Position-wise feed forward neural network\n",
        "    self.feed_forward_nn= FeedForwardNN(d_model, d_ff, dropout_rate)\n",
        "\n",
        "    # Layer normalization\n",
        "    self.norm= nn.LayerNorm(\n",
        "        normalized_shape= d_model\n",
        "    )\n",
        "\n",
        "    self.dropout= nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, enc_out, enc_mask, dec_mask):\n",
        "\n",
        "    # Step 1: Masked self-attention on decoder input\n",
        "    self_attention_output= self.self_attention(x, x, x, dec_mask)\n",
        "    self_attention_add_norm_output= self.norm(x + self.dropout(self_attention_output))\n",
        "\n",
        "    # Step 2: Cross-attention over encoder output\n",
        "    cross_attention_output= self.cross_attention(self_attention_add_norm_output, enc_out, enc_out, enc_mask)\n",
        "    cross_attention_add_norm_output= self.norm(cross_attention_output + self.dropout(self_attention_add_norm_output))\n",
        "\n",
        "    # Step 3: Feed Forward Network + Add & Norm\n",
        "    nn_output= self.feed_forward_nn(cross_attention_add_norm_output)\n",
        "    nn_add_norm_out= self.norm(cross_attention_add_norm_output + self.dropout(nn_output))\n",
        "\n",
        "    return nn_add_norm_out\n",
        "\n"
      ],
      "metadata": {
        "id": "UwdtD92V0tGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_ff, num_heads, dropout_rate, num_decoder_blocks):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.decoder_blocks= nn.ModuleList([])\n",
        "\n",
        "    for _ in range(num_decoder_blocks):\n",
        "      self.decoder_blocks.append(\n",
        "          DecoderBlock(d_model, d_ff, num_heads, dropout_rate)\n",
        "      )\n",
        "\n",
        "  def forward(self, x, enc_out, enc_mask, dec_mask):\n",
        "\n",
        "    # Pass input through each decoder block sequentially\n",
        "    for block in self.decoder_blocks:\n",
        "      x= block(x, enc_out, enc_mask, dec_mask)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "lvi7LYa30tC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingToVocabProjection(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, vocab_size):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense= nn.Linear(\n",
        "        in_features= d_model,\n",
        "        out_features= vocab_size\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x -> (B, S, d_model)\n",
        "\n",
        "    out= self.dense(x) # (B, S, vocab_size)\n",
        "    return torch.log_softmax(out, dim= -1)\n"
      ],
      "metadata": {
        "id": "nZG5ev6V0tAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KF00VKrH0s-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer architecture"
      ],
      "metadata": {
        "id": "ZEyt4nDk9T9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, enc_input, enc_pos, dec_input, dec_pos, final_projection):\n",
        "    super().__init__()\n",
        "\n",
        "    # Encoder model consisting of multiple EncoderBlocks\n",
        "    self.encoder_model = encoder\n",
        "\n",
        "    # Decoder model consisting of multiple DecoderBlocks\n",
        "    self.decoder_model = decoder\n",
        "\n",
        "    # Encoder: embedding layer for input tokens\n",
        "    self.enc_input_model = enc_input\n",
        "\n",
        "    # Encoder: positional encoding layer for input tokens\n",
        "    self.enc_pos_model = enc_pos\n",
        "\n",
        "    # Decoder: embedding layer for target tokens\n",
        "    self.dec_input_model = dec_input\n",
        "\n",
        "    # Decoder: positional encoding layer for target tokens\n",
        "    self.dec_pos_model = dec_pos\n",
        "\n",
        "    # Final linear projection layer to convert decoder output to vocabulary logits\n",
        "    self.final_projection_model = final_projection\n",
        "\n",
        "  def encode(self, enc_input, enc_mask):\n",
        "    # Generate token embeddings for encoder input\n",
        "    enc_embeddings = self.enc_input_model(enc_input)\n",
        "\n",
        "    # Add positional embeddings\n",
        "    enc_positional_embeddings = self.enc_pos_model(enc_embeddings)\n",
        "\n",
        "    # Pass through encoder\n",
        "    enc_output = self.encoder_model(enc_positional_embeddings, enc_mask)\n",
        "\n",
        "    return enc_output\n",
        "\n",
        "  def decode(self, dec_input, enc_output, enc_mask, dec_mask):\n",
        "    # Generate token embeddings for decoder input\n",
        "    dec_embeddings = self.dec_input_model(dec_input)\n",
        "\n",
        "    # Add positional embeddings\n",
        "    dec_positional_embeddings = self.dec_pos_model(dec_embeddings)\n",
        "\n",
        "    # Pass through decoder\n",
        "    dec_output = self.decoder_model(dec_positional_embeddings, enc_output, enc_mask, dec_mask)\n",
        "\n",
        "    return dec_output\n",
        "\n",
        "  def final_projection(self, dec_output):\n",
        "    # Project decoder output to vocabulary logits\n",
        "    return self.final_projection_model(dec_output)\n"
      ],
      "metadata": {
        "id": "VKTwbhNA9W4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNlTHLPK9W04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    if m.bias is not None:\n",
        "      nn.init.zeros_(m.bias)"
      ],
      "metadata": {
        "id": "JgrjpOKEFLSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1R3hojvFLIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size, trg_vocab_size, src_seq_len, trg_seq_len, d_model= 512, d_ff= 2048, num_heads= 8, num_enc_dec_blocks= 6, dropout_rate= 0.1):\n",
        "\n",
        "  # src -> ENC\n",
        "  # trg -> DEC\n",
        "\n",
        "  # create the input embeddings\n",
        "  enc_embedding= InputEmbeddings(d_model, src_vocab_size)\n",
        "  dec_embedding= InputEmbeddings(d_model, trg_vocab_size)\n",
        "\n",
        "  # create the positional embeddings\n",
        "  enc_pos_embedding= PositinalEncodings(d_model, src_seq_len, dropout_rate)\n",
        "  dec_pos_embedding= PositinalEncodings(d_model, trg_seq_len, dropout_rate)\n",
        "\n",
        "  # create the encoder and decoder\n",
        "  encoder_model= Encoder(d_model, d_ff, num_heads, dropout_rate, num_enc_dec_blocks)\n",
        "  decoder_model= Decoder(d_model, d_ff, num_heads, dropout_rate, num_enc_dec_blocks)\n",
        "\n",
        "  # create the final projection layer\n",
        "  final_projection_model= EmbeddingToVocabProjection(d_model, trg_vocab_size)\n",
        "\n",
        "  # create the transformer\n",
        "  transformer_model= Transformer(encoder_model, decoder_model, enc_embedding, enc_pos_embedding, dec_embedding, dec_pos_embedding, final_projection_model)\n",
        "\n",
        "  # weight init.\n",
        "  transformer_model.apply(lambda m: init_weights(m))\n",
        "\n",
        "  return transformer_model\n"
      ],
      "metadata": {
        "id": "5WPhDft40s8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQ8rRamIykqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizations"
      ],
      "metadata": {
        "id": "y5TksYLPLeRG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "tmeuatuEZnuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"cfilt/iitb-english-hindi\", split= \"train\")\n",
        "\n",
        "ds= ds.select(range(15000))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-Opal8M3K8q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0]"
      ],
      "metadata": {
        "id": "HRiOzRDlLK1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds)"
      ],
      "metadata": {
        "id": "sbv64d_Gp7ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcjWpbRqp8T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence extractor\n",
        "def get_all_sentences(ds, lang):\n",
        "  for item in ds:\n",
        "    yield item[\"translation\"][lang]"
      ],
      "metadata": {
        "id": "Qk5XcwvjLXk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gxJdiYapFDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build or load a tokenizer for a specific language dataset\n",
        "def build_tokenizer(ds, lang, save_path):\n",
        "  path = Path(save_path)\n",
        "\n",
        "  # If tokenizer file already exists, load it\n",
        "  if path.exists():\n",
        "    tokenizer = Tokenizer.from_file(str(path))\n",
        "\n",
        "  else:\n",
        "    # Create a new tokenizer with WordLevel model and [UNK] as the unknown token\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "\n",
        "    # Use whitespace to split text during tokenization\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    # Define trainer with special tokens and minimum frequency threshold\n",
        "    trainer = WordLevelTrainer(\n",
        "      special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],  # Special tokens\n",
        "      min_frequency=1  # Keep all tokens that appear at least once\n",
        "    )\n",
        "\n",
        "    # Train the tokenizer on all sentences in the dataset for the given language\n",
        "    tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "\n",
        "    # Save the trained tokenizer to file\n",
        "    tokenizer.save(str(path))\n",
        "\n",
        "  return tokenizer  # Return the loaded or newly trained tokenizer\n"
      ],
      "metadata": {
        "id": "TxeO3V36oC8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RjVXLxnoC5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyRJJ1UToC2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the tokenizers\n",
        "tokenizer_src = build_tokenizer(ds, 'en', '/content/tokenizer_en.json')\n",
        "tokenizer_trg = build_tokenizer(ds, 'hi', '/content/tokenizer_hi.json')\n"
      ],
      "metadata": {
        "id": "fykjlFdCoCz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_src.token_to_id(\"[UNK]\")"
      ],
      "metadata": {
        "id": "1zCXXcFloCxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = \"This is a sentence.\"\n",
        "text_hi = \"यह एक वाक्य है।\"\n",
        "\n",
        "print(\"English tokens:\", tokenizer_src.encode(text_en).ids)\n",
        "print(\"Hindi tokens:\", tokenizer_trg.encode(text_hi).ids)\n"
      ],
      "metadata": {
        "id": "mZyeRZj0rXQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NBSBWtQsE-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_src_len = 0\n",
        "max_trg_len = 0\n",
        "\n",
        "for item in ds:\n",
        "  src_text = item['translation']['en']\n",
        "  trg_text = item['translation']['hi']\n",
        "\n",
        "  src_len = len(tokenizer_src.encode(src_text).ids)\n",
        "  trg_len = len(tokenizer_trg.encode(trg_text).ids)\n",
        "\n",
        "  max_src_len = max(max_src_len, src_len)\n",
        "  max_trg_len = max(max_trg_len, trg_len)\n",
        "\n",
        "# Decide final seq_len (with SOS and EOS)\n",
        "final_seq_len = max(max_src_len, max_trg_len) + 2  # Add SOS and EOS\n",
        "print(\"Recommended seq_len:\", final_seq_len)"
      ],
      "metadata": {
        "id": "fX1SkuAS7Cq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "MzLOjRT3t4m4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPH9A9kN5YnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(size):\n",
        "  mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int64)\n",
        "  return (mask == 0)\n"
      ],
      "metadata": {
        "id": "yyNTM7-K5H6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "causal_mask(5).shape, causal_mask(5)"
      ],
      "metadata": {
        "id": "gBOENQXu5bIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gChOszao5bEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom PyTorch dataset class for sequence-to-sequence translation tasks\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, ds, tokenizer_src, tokenizer_trg, seq_len, src_lang=\"en\", trg_lang=\"hi\"):\n",
        "    self.ds = ds  # Hugging Face translation dataset\n",
        "    self.tokenizer_src = tokenizer_src  # Source language tokenizer\n",
        "    self.tokenizer_trg = tokenizer_trg  # Target language tokenizer\n",
        "    self.seq_len = seq_len  # Maximum sequence length\n",
        "\n",
        "    self.src_lang = src_lang  # Source language code\n",
        "    self.trg_lang = trg_lang  # Target language code\n",
        "\n",
        "    # Special tokens\n",
        "    self.sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")])\n",
        "    self.eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")])\n",
        "    self.pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ds)  # Total number of samples\n",
        "\n",
        "  def __getitem__(self, id):\n",
        "    # Get translation pair from dataset\n",
        "    src_trg_pair = ds[id][\"translation\"]\n",
        "\n",
        "    # Extract source and target texts\n",
        "    src_text = src_trg_pair[self.src_lang]\n",
        "    trg_text = src_trg_pair[self.trg_lang]\n",
        "\n",
        "    # Tokenize the source and target sentences\n",
        "    src_token_ids = self.tokenizer_src.encode(src_text).ids\n",
        "    trg_token_ids = self.tokenizer_trg.encode(trg_text).ids\n",
        "\n",
        "    # Compute padding lengths (subtracting SOS/EOS tokens)\n",
        "    src_num_padding = self.seq_len - len(src_token_ids) - 2  # SOS + EOS\n",
        "    trg_num_padding = self.seq_len - len(trg_token_ids) - 1  # Only EOS\n",
        "\n",
        "    if src_num_padding < 0 or trg_num_padding < 0:\n",
        "      print(f\"Too long sentence :(\")  # Optionally handle truncation here\n",
        "\n",
        "    # Create encoder input: [SOS] + tokens + [EOS] + [PAD]...\n",
        "    enc_inputs = torch.cat([\n",
        "        self.sos_token,\n",
        "        torch.tensor(src_token_ids, dtype=torch.int64),\n",
        "        self.eos_token,\n",
        "        torch.tensor([self.pad_token] * src_num_padding, dtype=torch.int64)\n",
        "    ], dim=0)\n",
        "\n",
        "    # Create decoder input: [SOS] + tokens + [PAD]...\n",
        "    dec_inputs = torch.cat([\n",
        "        self.sos_token,\n",
        "        torch.tensor(trg_token_ids, dtype=torch.int64),\n",
        "        torch.tensor([self.pad_token] * trg_num_padding, dtype=torch.int64)\n",
        "    ], dim=0)\n",
        "\n",
        "    # Create decoder target labels: tokens + [EOS] + [PAD]...\n",
        "    labels = torch.cat([\n",
        "        torch.tensor(trg_token_ids, dtype=torch.int64),\n",
        "        self.eos_token,\n",
        "        torch.tensor([self.pad_token] * trg_num_padding, dtype=torch.int64)\n",
        "    ], dim=0)\n",
        "\n",
        "    # Return the sample as a dictionary\n",
        "    return {\n",
        "      \"enc_inputs\": enc_inputs,  # Input to encoder\n",
        "      \"dec_inputs\": dec_inputs,  # Input to decoder\n",
        "      \"labels\": labels,          # Target output\n",
        "      \"src_text\": src_text,      # Raw source text\n",
        "      \"trg_text\": trg_text,      # Raw target text\n",
        "      \"encoder_mask\": (enc_inputs != self.pad_token).unsqueeze(0).unsqueeze(0).int(),  # 3D encoder mask\n",
        "      \"decoder_mask\": (dec_inputs != self.pad_token).unsqueeze(0).int() & causal_mask(dec_inputs.size(0))  # Combined decoder mask\n",
        "    }\n"
      ],
      "metadata": {
        "id": "LDn8ZAAot4XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUtsGAXZt4UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the raw ds\n",
        "\n",
        "ds_len= len(ds)\n",
        "train_ds_len= int(0.80 * ds_len)\n",
        "valid_ds_len= int(0.15 * ds_len)\n",
        "test_ds_len= ds_len - train_ds_len - valid_ds_len\n",
        "\n",
        "train_ds, valid_ds, test_ds= random_split(ds, [train_ds_len, valid_ds_len, test_ds_len])"
      ],
      "metadata": {
        "id": "IffqOaOmt4RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the datasets -> train and valid\n",
        "\n",
        "train_dataset= CustomDataset(train_ds, tokenizer_src, tokenizer_trg, final_seq_len)\n",
        "valid_dataset= CustomDataset(valid_ds, tokenizer_src, tokenizer_trg, final_seq_len)\n",
        "test_dataset= CustomDataset(test_ds, tokenizer_src, tokenizer_trg, final_seq_len)"
      ],
      "metadata": {
        "id": "C59i7gwM8BLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the data loaders -> train and valid\n",
        "\n",
        "BATCH_SIZE= 8\n",
        "\n",
        "train_loader= DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True, pin_memory= True)\n",
        "valid_loader= DataLoader(valid_dataset, batch_size= BATCH_SIZE, shuffle= True, pin_memory= True)\n",
        "test_loader= DataLoader(test_dataset, batch_size= 1, shuffle= True, pin_memory= True)"
      ],
      "metadata": {
        "id": "FdRzme_D8ZIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8jiIr9fF8rEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic= next(iter(train_loader))\n",
        "\n",
        "dic['enc_inputs'].shape, dic['dec_inputs'].shape, dic[\"labels\"].shape, len(dic[\"src_text\"]), len(dic[\"trg_text\"]), dic[\"encoder_mask\"].shape, dic[\"decoder_mask\"].shape"
      ],
      "metadata": {
        "id": "E02Mo7vm8-D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4MxOdhm29Bfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training inits."
      ],
      "metadata": {
        "id": "2gt7GEyfB4CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src vocab size -> tokenizer_src.get_vocab_size()\n",
        "# dest vocab size -> tokenizer_trg.get_vocab_size()"
      ],
      "metadata": {
        "id": "Cj5FrNF2D0cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHlnJctjD66B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the transformer model\n",
        "\n",
        "model= build_transformer(tokenizer_src.get_vocab_size(), tokenizer_trg.get_vocab_size(), final_seq_len, final_seq_len, num_heads= 4, num_enc_dec_blocks= 4)\n",
        "model= model.to(DEVICE)"
      ],
      "metadata": {
        "id": "OnxYhzfYB3od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gUmID3t6B3lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zp5jVsCFB3i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opt and loss\n",
        "\n",
        "optimizer= optim.Adam(model.parameters(), lr= lr, weight_decay= weight_decay)\n",
        "loss_func= nn.CrossEntropyLoss(ignore_index= tokenizer_src.token_to_id('[PAD]'), label_smoothing= 0.1)"
      ],
      "metadata": {
        "id": "RuT5FIY-B3dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ApJzHncGclV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses= []\n",
        "valid_losses= []\n",
        "\n",
        "len(train_loader)"
      ],
      "metadata": {
        "id": "hnf41uxYGrGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_trg.get_vocab_size()"
      ],
      "metadata": {
        "id": "xe-KqLqYJ2gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "u2FpJooUSROT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(valid_loader), len(test_loader)"
      ],
      "metadata": {
        "id": "j3CDXesyLV8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(valid_dataset), len(test_dataset)"
      ],
      "metadata": {
        "id": "UzcqtoM6LYen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arqpIJODLYbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_validation_model(model):\n",
        "\n",
        "  best_epoch= 0\n",
        "  min_valid_loss= float(\"inf\")\n",
        "  transformer_best_weights_path= \"/content/weights.pth\"\n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"\\nRunning epoch [{epoch + 1}/{num_epochs}]:-\\n\\n\")\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss= 0\n",
        "\n",
        "    train_loop= tqdm(train_loader, desc= \"Training\", total= len(train_loader))\n",
        "\n",
        "    batch_id= 0\n",
        "\n",
        "    for batch in train_loop:\n",
        "\n",
        "      # get the inputs\n",
        "      enc_inputs= batch[\"enc_inputs\"].to(DEVICE) # (b, seq_len)\n",
        "      dec_inputs= batch[\"dec_inputs\"].to(DEVICE) # (b, seq_len)\n",
        "      labels= batch[\"labels\"].to(DEVICE) # (b, seq_len)\n",
        "      src_text= batch[\"src_text\"] # (b)\n",
        "      trg_text= batch[\"trg_text\"] # (b)\n",
        "      encoder_mask= batch[\"encoder_mask\"].to(DEVICE) # (b, 1, 1, seq_len)\n",
        "      decoder_mask= batch[\"decoder_mask\"].to(DEVICE) # (b, 1, seq_len, seq_len)\n",
        "\n",
        "      # calc the model preds\n",
        "      enc_outputs= model.encode(enc_inputs, encoder_mask) # (b, seq_len, d_model)\n",
        "      dec_outputs= model.decode(dec_inputs, enc_outputs, encoder_mask, decoder_mask) # (b, seq_len, d_model)\n",
        "      final_projections= model.final_projection(dec_outputs) # (b, seq_len, vocab_size)\n",
        "\n",
        "      # calc loss:= (b * seq_len, vocab_size) -> (b * seq_len)\n",
        "      loss= loss_func(final_projections.view(-1, final_projections.shape[-1]), labels.view(-1))\n",
        "\n",
        "      # update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      # display\n",
        "      train_loop.set_postfix({\n",
        "          \"loss\": f\"{loss.item():.4f}\"\n",
        "      })\n",
        "\n",
        "      if (batch_id + 1) % TRAIN_CONSOLE_CHECKPOINT == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] ~ Batch [{batch_id + 1}] -> Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "      total_train_loss+= loss.item()\n",
        "\n",
        "      batch_id+= 1\n",
        "\n",
        "    avg_train_loss= total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # validation loop\n",
        "    model.eval()\n",
        "\n",
        "    total_valid_loss= 0\n",
        "\n",
        "    valid_loop= tqdm(valid_loader, desc= \"Validation\", total= len(valid_loader))\n",
        "\n",
        "    batch_id= 0\n",
        "\n",
        "    for batch in valid_loop:\n",
        "\n",
        "      # get the inputs\n",
        "      enc_inputs= batch[\"enc_inputs\"].to(DEVICE) # (b, seq_len)\n",
        "      dec_inputs= batch[\"dec_inputs\"].to(DEVICE) # (b, seq_len)\n",
        "      labels= batch[\"labels\"].to(DEVICE) # (b, seq_len)\n",
        "      src_text= batch[\"src_text\"] # (b)\n",
        "      trg_text= batch[\"trg_text\"] # (b)\n",
        "      encoder_mask= batch[\"encoder_mask\"].to(DEVICE) # (b, 1, 1, seq_len)\n",
        "      decoder_mask= batch[\"decoder_mask\"].to(DEVICE) # (b, 1, seq_len, seq_len)\n",
        "\n",
        "      # calc the model preds\n",
        "      with torch.no_grad():\n",
        "        enc_outputs= model.encode(enc_inputs, encoder_mask) # (b, seq_len, d_model)\n",
        "        dec_outputs= model.decode(dec_inputs, enc_outputs, encoder_mask, decoder_mask) # (b, seq_len, d_model)\n",
        "        final_projections= model.final_projection(dec_outputs) # (b, seq_len, vocab_size)\n",
        "\n",
        "        # calc loss:= (b * seq_len, vocab_size) -> (b * seq_len)\n",
        "        loss= loss_func(final_projections.view(-1, final_projections.shape[-1]), labels.view(-1))\n",
        "\n",
        "      # display\n",
        "      valid_loop.set_postfix({\n",
        "          \"loss\": f\"{loss.item():.4f}\"\n",
        "      })\n",
        "\n",
        "      if (batch_id + 1) % VALID_CONSOLE_CHECKPOINT == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] ~ Batch [{batch_id + 1}] -> Valid Loss: {loss.item():.4f}\")\n",
        "\n",
        "      total_valid_loss+= loss.item()\n",
        "\n",
        "      batch_id+= 1\n",
        "\n",
        "    avg_valid_loss= total_valid_loss / len(valid_loader)\n",
        "\n",
        "    # save best weights\n",
        "    if avg_valid_loss < min_valid_loss:\n",
        "      min_valid_loss= avg_valid_loss\n",
        "      best_epoch= epoch\n",
        "      torch.save(model.state_dict(), transformer_best_weights_path)  # Save model weights\n",
        "\n",
        "\n",
        "    valid_losses.append(avg_valid_loss)\n",
        "    print(f\"\\n\\nEpoch [{epoch + 1}/{num_epochs}] -> Avg Train Loss: {avg_train_loss:.4f} ~ Avg Valid Loss: {avg_valid_loss:.4f}\\n\\n\")\n",
        "\n",
        "    # load best weights if its the last epoch\n",
        "    if (epoch + 1) == num_epochs:\n",
        "\n",
        "      model= build_transformer(tokenizer_src.get_vocab_size(), tokenizer_trg.get_vocab_size(), final_seq_len, final_seq_len, num_heads= 4, num_enc_dec_blocks= 3)\n",
        "      model.load_state_dict(torch.load(transformer_best_weights_path))  # Load saved weights\n",
        "      model= model.to(DEVICE)\n",
        "\n",
        "      print(f\"\\n\\nTraining done, loading the best weights...\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BoAmfzqDSQ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_validation_model(model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fkrOe0ZwSQ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M57cGozbSQ6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-VJQBkRPXTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot train and validation metrics"
      ],
      "metadata": {
        "id": "NUcVQjzEPjX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, marker='o', linestyle='-', color='blue', label='Avg Train Loss')\n",
        "plt.plot(valid_losses, marker='s', label='Avg Validation Loss', color='orange')\n",
        "plt.title(\"Avg Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dfcjxEc8SQ4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Upfcnr6DSQ12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer inference"
      ],
      "metadata": {
        "id": "0_3EU4jsVB7k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8VmOLpaoyh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding function for inference (generates one token at a time)\n",
        "def greedy_decode(encoder_inputs, encoder_mask):\n",
        "\n",
        "  # Get the special token IDs\n",
        "  sos_id = tokenizer_src.token_to_id('[SOS]')  # Start of sentence\n",
        "  eos_id = tokenizer_src.token_to_id('[EOS]')  # End of sentence\n",
        "\n",
        "  # Run the encoder to get encoder outputs\n",
        "  encoder_outputs = model.encode(encoder_inputs, encoder_mask)\n",
        "\n",
        "  # Initialize decoder input with only SOS token: shape (1, 1)\n",
        "  decoder_inputs = torch.empty((1, 1)).fill_(sos_id).type_as(encoder_inputs).to(DEVICE)\n",
        "\n",
        "  while True:\n",
        "    # Stop if the sequence becomes too long\n",
        "    if decoder_inputs.shape[1] >= final_seq_len:\n",
        "      break\n",
        "\n",
        "    # Create the causal (triangular) mask for decoder (prevents seeing future tokens)\n",
        "    decoder_mask = causal_mask(decoder_inputs.shape[1]).type_as(encoder_mask).to(DEVICE)\n",
        "\n",
        "    # Run the decoder to get output logits\n",
        "    decoder_outputs = model.decode(decoder_inputs, encoder_outputs, encoder_mask, decoder_mask)\n",
        "\n",
        "    # Get the final (last time step) logits from decoder\n",
        "    probs = model.final_projection(decoder_outputs[:, -1])  # Shape: (1, vocab_size)\n",
        "\n",
        "    # Choose the token with the highest probability (greedy)\n",
        "    _, highest_prob_index = torch.max(probs, dim=1)  # Shape: (1,)\n",
        "\n",
        "    # Append predicted token to decoder input for next step\n",
        "    decoder_inputs = torch.cat([\n",
        "        decoder_inputs,\n",
        "        torch.empty((1, 1)).fill_(highest_prob_index.item()).type_as(encoder_inputs).to(DEVICE)\n",
        "    ], dim=1)\n",
        "\n",
        "    # Stop if EOS token is generated\n",
        "    if highest_prob_index == eos_id:\n",
        "      break\n",
        "\n",
        "  # Return the generated token sequence (excluding batch dim)\n",
        "  return decoder_inputs.squeeze(0)\n"
      ],
      "metadata": {
        "id": "RxekXZMSha-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0p518kFKv2A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference loop to evaluate the Transformer model on test data\n",
        "def transformer_inference():\n",
        "\n",
        "  # Set model to evaluation mode (disables dropout, etc.)\n",
        "  model.eval()\n",
        "\n",
        "  # Create a tqdm progress bar for test_loader\n",
        "  test_loop = tqdm(test_loader, desc=\"Testing\", total=len(test_loader))\n",
        "\n",
        "  # Initialize storage for logging results\n",
        "  source_texts = []     # Source sentences (English)\n",
        "  target_texts = []     # Ground truth target sentences (Hindi)\n",
        "  predicted_texts = []  # Model-generated translations\n",
        "\n",
        "  for batch in test_loop:\n",
        "\n",
        "    # Move tensors to DEVICE (e.g., GPU)\n",
        "    enc_inputs = batch[\"enc_inputs\"].to(DEVICE)           # Encoder input sequence (1, seq_len)\n",
        "    dec_inputs = batch[\"dec_inputs\"].to(DEVICE)           # Decoder input sequence (1, seq_len)\n",
        "    labels = batch[\"labels\"].to(DEVICE)                   # Ground truth target sequence (1, seq_len)\n",
        "    src_text = batch[\"src_text\"]                          # Source sentence as string (list of one string)\n",
        "    trg_text = batch[\"trg_text\"]                          # Target sentence as string (list of one string)\n",
        "    encoder_mask = batch[\"encoder_mask\"].to(DEVICE)       # Encoder attention mask (1, 1, 1, seq_len)\n",
        "    decoder_mask = batch[\"decoder_mask\"].to(DEVICE)       # Decoder attention mask (1, 1, seq_len, seq_len)\n",
        "\n",
        "    # Get prediction from the greedy decoder\n",
        "    model_out = greedy_decode(enc_inputs, encoder_mask)\n",
        "\n",
        "    # Collect original and predicted texts for comparison\n",
        "    source_texts.append(src_text[0])  # English sentence\n",
        "    target_texts.append(trg_text[0])  # Actual Hindi sentence\n",
        "    predicted_texts.append(tokenizer_trg.decode(model_out.detach().cpu().numpy()))  # Predicted Hindi\n",
        "\n",
        "  # Print all translations: source, target (ground truth), and predicted\n",
        "  for i in range(len(source_texts)):\n",
        "    print(f\"\\nSRC: {source_texts[i]}\")  # Original English input\n",
        "    print(f\"TRG: {target_texts[i]}\")    # Ground truth Hindi translation\n",
        "    print(f\"PRD: {predicted_texts[i]}\") # Model's Hindi prediction\n",
        "    print(f\"\\n{'-' * 50}\")\n"
      ],
      "metadata": {
        "id": "QSgnTd5Pha4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_inference()"
      ],
      "metadata": {
        "id": "af-ms6Y5ha12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diigl8r6hazX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZq1AMqDhaw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UAlxhV4haun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SO5oWGZyhar9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
